# EPFG Overview

The epfg program uses Evolutionary Programming techniques to evolve a collection of Atari frame kernels to match the
input frames of a provided movie. It opens a web port on the running computer, allowing for remote monitoring of the
frame fitting process, which can take significant compute time to complete.

epfg must always be invoked with a --database_path argument, which points to a directory in which to store or access a
leveldb file database. If starting a new fit process, an --input_movie argument can also be provided, in which case the
--database_path argument is expected to be empty.

The overall fit process occurs in a series of stages, detailed here.

## Stage 0 - Database Initialization, Frame Extraction

Done only on a first run against a movie/database pair, epfg will extract all frames out into a TargetVideoFrame record
with timing, pixel, and keyframe information.

Then, on a second pass, epfg will group the frames into keyframe groups, and de-dupe any identical frames *within a
keyframe group only*. Research question - does ffmpeg produce identical subsequent frames? Or can one do a variable
framerate decode and be sure that you're only getting the new frames as they are produced? Looking at timestamps might
be a great way to determine this.

The end of Stage 0 is with the entire movie extracted into keyframe groups, each containing 1 or more TargetVideoFrame,
and any other structures set up in the file database as needed to support subsequent stages.

We will create a series of tasks, each task representing the order of a fitting session against a keyframe group. One
task for each keyframe group will be created. There's also a timestamped log stream that gets saved into the database.

## Processing of Individual Keyframe Groups

It can be a runtime parameter how many different keyframe groups vcsmc tries to fit at a time. Default could be 1.
Each stage is scheduled here at the job-per-frame level.

### Stage 1.0 - Random Generation of Initial Kernels

Each frame will have a population of kernels generated at random, fingerprinted, and saved into the database. Each
generation per frame also gets saved in the database in a frame-XXX-generation-XXX key.

### Stage 1.1 - Simulation and Scoring

Every kernel in the generation gets simulated, so we know the frame output, and then scored against the frame it was
made for. It should also be possible to score higher-performing kernels against other frames in the group. We rely
heavily on caching so scoring is hopefully cheaper, in that maybe regions of the target images are similar in a keyframe
group, and will be tested against the same kernel.

The competitive random selection can happen via tournament, but all kernels in the generation, as well as all additional
kernels selected for scoring from other frames, compete in the tournament. They are then sorted by score and the top
half of the fixed size generation is kept, with the bottom half marked for regeneration from kernels from the top half.

This doesn't stop for any frame until *all* frames either meet the generation cap count or meet the accuracy
requirement. If neither requirement is met we move on to Stage 1.2.

### Stage 1.2 - Next Generation Kernels

The top half is copied from the previous generation, and the bottom half is generated by a process of random mutation
from the top half kernels. Any unsimulated kernel is then simulated and scored, going back to Stage 1.1 of the process.


# Job System

We want a system of jobs that can create subtasks recursively. A job won't be considered finished until all subjobs it
has created finish. Top-level jobs can be executed in parallel, and an individual job can use multi-threaded processing
to fully saturate the computer. The idea of jobs is to allow relatively fine-level granularity in work reporting,
measuring, and resumability, so the overall process of fitting can be started and stopped without extensive re-work.

